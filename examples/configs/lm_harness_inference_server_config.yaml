# Model to evaluate specified as a local-chat-completions inference server
model:
  inference:
    base_url: "http://1.2.3.4:8000/v1"
    # HuggingFace config for the actual model being hosted
    engine:
      name: artifact-name
      project: artifact-project --> str
  # 'huggingface' or 'tiktoken' depending on model type
  tokenizer_backend: "huggingface"

# Settings specific to lm_harness.evaluate
evaluator:
  tasks: ["gsm8k"]
  num_fewshot: 5
  limit: 10

tracking:
  name: "flamingo-example-lm-harness-inference"
  project: "flamingo-examples"
  entity: "mozilla-ai"

ray:
  num_cpus: 1
  timeout: 3600
